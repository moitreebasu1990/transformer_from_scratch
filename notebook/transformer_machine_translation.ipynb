{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer Model for Machine Translation: A Comprehensive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Transfomer` architecture is one of the basic most influential and important contribution in the field of Natural Language Processing (NLP). In this notebook, we are going to demostrate the implementations of the Transformer-based translation model using PyTorch. The model is designed to translate text from one language to another. We'll cover every aspect in detail, from data preparation to model architecture, training, evaluation, and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pdeb/mambaforge/envs/MJ/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/pdeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/pdeb/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up the device for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else device\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'lr': 0.0001, 'bs': 16, 'n_epochs': 100, 'src_lang': 'en', 'tgt_lang': 'fr', 'model_path': './model/', 'dim_embed': 256, 'n_layers': 6, 'n_heads': 8, 'dropout': 0.1, 'ffn_hidden_dim': 256}\n"
     ]
    }
   ],
   "source": [
    "def get_params():\n",
    "    \"\"\"\n",
    "    Define and return a dictionary of hyperparameters and configuration settings.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing model and training parameters.\n",
    "    \"\"\"\n",
    "    param = {\n",
    "        'lr': 1e-4,  # Learning rate\n",
    "        'bs': 16,    # Batch size\n",
    "        'n_epochs': 100,  # Number of training epochs\n",
    "        'src_lang': 'en',  # Source language\n",
    "        'tgt_lang': 'fr',  # Target language\n",
    "        'model_path': './model/',  # Path to store model\n",
    "        'dim_embed': 256,  # Embedding dimension\n",
    "        'n_layers': 6,  # Number of encoder/decoder layers\n",
    "        'n_heads': 8,  # Number of attention heads\n",
    "        'dropout': 0.1,  # Dropout rate\n",
    "        'ffn_hidden_dim': 256  # Hidden dimension of feed-forward network\n",
    "    }\n",
    "    return param\n",
    "\n",
    "parameters = get_params()\n",
    "print(f\"Parameters: {parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation\n",
    "\n",
    "Now, let's prepare our dataset. We're using the 'opus_books' dataset, which contains English-French translations. You can select a subset(~2000) of examples to keep the training process manageable for your machine.\n",
    "\n",
    "We transform the dataset into a list of tuples of (source, target) pairs, where source is the English sentence and target is the French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pdeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/pdeb/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "# ds = load_dataset('opus_books', 'en-fr', split='train')\n",
    "ds = load_dataset('opus_books', 'en-fr', split='train').select(range(20000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from the dataset:\n",
      "[('The Wanderer', 'Le grand Meaulnes'), ('Alain-Fournier', 'Alain-Fournier')]\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset as a list of (source, target) pairs\n",
    "dataset = []\n",
    "for example in ds:\n",
    "    src = example['translation']['en']\n",
    "    tgt = example['translation']['fr']\n",
    "    dataset.append((src, tgt))\n",
    "\n",
    "print(\"Sample from the dataset:\")\n",
    "print(dataset[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary Creation\n",
    "\n",
    "The next step is to create vocabularies for both source and target languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocabulary size: 21051\n",
      "Target vocabulary size: 28348\n"
     ]
    }
   ],
   "source": [
    "def create_vocabulary(dataset):\n",
    "    \"\"\"\n",
    "    Create vocabularies for source and target languages.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): List of (source, target) sentence pairs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains vocab_src, vocab_tgt, src_token_to_id, tgt_token_to_id,\n",
    "               src_id_to_token, tgt_id_to_token\n",
    "    \"\"\"\n",
    "    vocab_src = set()\n",
    "    vocab_tgt = set()\n",
    "\n",
    "    max_src_seq_len = 0\n",
    "    max_tgt_seq_len = 0\n",
    "\n",
    "    for src, tgt in dataset:\n",
    "        max_src_seq_len = max(max_src_seq_len, len(src))\n",
    "        max_tgt_seq_len = max(max_tgt_seq_len, len(tgt))\n",
    "\n",
    "        src_tokens = word_tokenize(src)\n",
    "        tgt_tokens = word_tokenize(tgt)\n",
    "        vocab_src.update(src_tokens)\n",
    "        vocab_tgt.update(tgt_tokens)\n",
    "\n",
    "    # Add special tokens\n",
    "    special_tokens = ['<sos>', '<eos>', '<pad>', '<unk>']\n",
    "    vocab_src = list(vocab_src) + special_tokens\n",
    "    vocab_tgt = list(vocab_tgt) + special_tokens\n",
    "\n",
    "    vocab_src_size = len(vocab_src)\n",
    "    vocab_tgt_size = len(vocab_tgt)\n",
    "\n",
    "    print(f\"Source vocabulary size: {vocab_src_size}\")\n",
    "    print(f\"Target vocabulary size: {vocab_tgt_size}\")\n",
    "\n",
    "    # Create token to id and id to token mappings\n",
    "    src_token_to_id = {token: id for id, token in enumerate(vocab_src)}\n",
    "    tgt_token_to_id = {token: id for id, token in enumerate(vocab_tgt)}\n",
    "    src_id_to_token = {id: token for token, id in src_token_to_id.items()}\n",
    "    tgt_id_to_token = {id: token for token, id in tgt_token_to_id.items()}\n",
    "\n",
    "    return (vocab_src, vocab_src_size, max_src_seq_len, vocab_tgt, vocab_tgt_size, max_tgt_seq_len, src_token_to_id, tgt_token_to_id,\n",
    "            src_id_to_token, tgt_id_to_token)\n",
    "\n",
    "vocab_src, vocab_src_size, max_src_seq_len, vocab_tgt, vocab_tgt_size, max_tgt_seq_len, src_token_to_id, tgt_token_to_id, src_id_to_token, tgt_id_to_token = create_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Printing src_token_to_id\n",
    "# print(\"\\nsrc_token_to_id:\")\n",
    "# print(src_token_to_id)\n",
    "\n",
    "# # Printing tgt_token_to_id\n",
    "# print(\"\\ntgt_token_to_id:\")\n",
    "# print(tgt_token_to_id)\n",
    "\n",
    "# # Printing src_id_to_token\n",
    "# print(\"\\nsrc_id_to_token:\")\n",
    "# print(src_id_to_token)\n",
    "\n",
    "# # Printing tgt_id_to_token\n",
    "# print(\"\\ntgt_id_to_token:\")\n",
    "# print(tgt_id_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This above function accomplishes few important things:\n",
    "\n",
    "1. Tokenizes each sentence in the dataset.\n",
    "2. Creates vocabularies for both source (`en`) and target (`fr`) languages by collecting all unique words.\n",
    "3. Adds special tokens like `<sos>` (start of sentence), `<eos>` (end of sentence), `<pad>` (padding), and `<unk>` (unknown words).\n",
    "4. Creates mappings between tokens and their numerical IDs, which we'll use to convert sentences into sequences of numbers that our model can process.\n",
    "\n",
    "## 5. Custom Dataset and DataLoader\n",
    "\n",
    "Now, we create a custom Dataset class and a collate function for our DataLoader:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Source shape: torch.Size([2, 12])\n",
      "Target shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for our translation task.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (list): List of (source, target) sentence pairs.\n",
    "        src_token_to_id (dict): Mapping from source tokens to IDs.\n",
    "        tgt_token_to_id (dict): Mapping from target tokens to IDs.\n",
    "        src_id_to_token (dict): Mapping from source IDs to tokens.\n",
    "        tgt_id_to_token (dict): Mapping from target IDs to tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, src_token_to_id, tgt_token_to_id, src_id_to_token, tgt_id_to_token):\n",
    "        self.dataset = dataset\n",
    "        self.src_token_to_id = src_token_to_id\n",
    "        self.tgt_token_to_id = tgt_token_to_id\n",
    "        self.src_id_to_token = src_id_to_token\n",
    "        self.tgt_id_to_token = tgt_id_to_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetch and process a single item from the dataset.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the item to fetch.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Contains source and target tensor, each representing a sentence as a sequence of token IDs.\n",
    "        \"\"\"\n",
    "        src, tgt = self.dataset[idx]\n",
    "        # Tokenize and add start/end tokens\n",
    "        src_tokens = ['<sos>'] + word_tokenize(src) + ['<eos>']\n",
    "        tgt_tokens = ['<sos>'] + word_tokenize(tgt) + ['<eos>']\n",
    "        # Convert tokens to IDs\n",
    "        src_ids = [self.src_token_to_id.get(token, self.src_token_to_id['<unk>']) for token in src_tokens]\n",
    "        tgt_ids = [self.tgt_token_to_id.get(token, self.tgt_token_to_id['<unk>']) for token in tgt_tokens]\n",
    "        return torch.tensor(src_ids).long(), torch.tensor(tgt_ids).long()\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to create batches of data.\n",
    "\n",
    "    This function pads sequences in a batch to the same length.\n",
    "\n",
    "    Args:\n",
    "        batch (list): List of (source, target) tensor pairs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains padded source and target tensors.\n",
    "    \"\"\"\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_max_len = max(len(src) for src in src_batch)\n",
    "    tgt_max_len = max(len(tgt) for tgt in tgt_batch)\n",
    "\n",
    "    # Pad sequences\n",
    "    src_batch = [torch.cat([src, torch.full((src_max_len - len(src),), src_token_to_id['<pad>'])]) for src in src_batch]\n",
    "    tgt_batch = [torch.cat([tgt, torch.full((tgt_max_len - len(tgt),), tgt_token_to_id['<pad>'])]) for tgt in tgt_batch]\n",
    "\n",
    "    # Stack tensors\n",
    "    src_batch = torch.stack(src_batch)\n",
    "    tgt_batch = torch.stack(tgt_batch)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TranslationDataset(dataset, src_token_to_id, tgt_token_to_id, src_id_to_token, tgt_id_to_token)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Print a sample batch\n",
    "for batch_idx, (src, tgt) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"Source shape:\", src.shape)\n",
    "    print(\"Target shape:\", tgt.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TranslationDataset` class is a custom implementation of PyTorch's Dataset. It handles the conversion of our text data into tensor form. The `__getitem__` method is particularly important as it:\n",
    "\n",
    "1. Retrieves a source-target sentence pair\n",
    "2. Tokenizes both sentences\n",
    "3. Adds start and end tokens\n",
    "4. Converts tokens to their corresponding IDs\n",
    "\n",
    "The `collate_fn` function is used by the DataLoader to process our data into batches. It ensures that all sequences in a batch are padded to the same length, which is necessary for efficient processing by our model.\n",
    "\n",
    "This setup allows us to easily iterate over our data during training, with each batch containing padded sequences of token IDs.\n",
    "\n",
    "In the next sections, we'll dive into the model architecture, starting with the individual components of the Transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Architecture\n",
    "\n",
    "Now, let's dive into the heart of our Transformer model. We'll break down each component, explaining its purpose and implementation in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds input tokens into continuous vector representations.\n",
    "\n",
    "    Attributes:\n",
    "        dim_embed (int): The dimension of the embedding space.\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        embedding (nn.Embedding): The embedding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, dim_embed: int):\n",
    "        \"\"\"\n",
    "        Initialize the InputEmbedding.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "            dim_embed (int): The dimension of the embedding space.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embed = dim_embed\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.dim_embed)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embed the input tokens.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of token IDs.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Embedded representation of the input.\n",
    "        \"\"\"\n",
    "        input = input.long()\n",
    "        # Scale embeddings by sqrt(dim_embed) as per the original paper\n",
    "        output = self.embedding(input) * math.sqrt(self.dim_embed)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `InputEmbedding` class is responsible for converting our input tokens (represented as integer IDs) into continuous vector representations. Here's what's happening:\n",
    "\n",
    "1. We initialize an `nn.Embedding` layer with our vocabulary size and embedding dimension.\n",
    "2. In the forward pass, we ensure our input is of type `long` (required for `nn.Embedding`).\n",
    "3. We multiply the embeddings by the square root of the embedding dimension. This scaling is mentioned in the original Transformer paper and helps to keep the variance of the embeddings roughly constant regardless of the embedding dimension.\n",
    "\n",
    "### 6.2 Positional Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds positional information to input embeddings.\n",
    "\n",
    "    Attributes:\n",
    "        dim_embed (int): The dimension of the embedding space.\n",
    "        max_seq_len (int): The maximum sequence length.\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "        pos_emb (torch.Tensor): Pre-computed positional embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_embed: int, max_seq_len: int = 1000, dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the PositionalEmbedding.\n",
    "\n",
    "        Args:\n",
    "            dim_embed (int): The dimension of the embedding space.\n",
    "            max_seq_len (int, optional): The maximum sequence length. Defaults to 1000.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim_embed = dim_embed\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Pre-compute positional embeddings\n",
    "        pos_emb = torch.zeros(max_seq_len, dim_embed)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim_embed, 2).float() * (-math.log(10000.0) / dim_embed))\n",
    "\n",
    "        pos_emb[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_emb[:, 1::2] = torch.cos(position * div_term)\n",
    "        pos_emb = pos_emb.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pos_emb', pos_emb)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Add positional embeddings to the input.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim_embed).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Input with added positional embeddings.\n",
    "        \"\"\"\n",
    "        output = input + self.pos_emb[:, :input.size(1)].requires_grad_(False)\n",
    "        return self.dropout(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PositionalEmbedding` class adds positional information to our input embeddings. This is crucial because the Transformer model, unlike RNNs, doesn't inherently understand the order of its inputs. Here's what's happening:\n",
    "\n",
    "1. We pre-compute positional embeddings for positions up to `max_seq_len`.\n",
    "2. The positional embeddings use sine and cosine functions of different frequencies, as described in the original Transformer paper. This allows the model to easily learn to attend to relative positions.\n",
    "3. In the forward pass, we add these positional embeddings to our input. We only use as many positional embeddings as we have input tokens.\n",
    "4. We apply dropout for regularization.\n",
    "\n",
    "### 6.3 Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Layer Normalization.\n",
    "\n",
    "    Attributes:\n",
    "        eps (float): A small value added for numerical stability.\n",
    "        alpha (nn.Parameter): Learnable scale parameter.\n",
    "        bias (nn.Parameter): Learnable bias parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize LayerNormalization.\n",
    "\n",
    "        Args:\n",
    "            eps (float, optional): Small value added for numerical stability. Defaults to 1e-6.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # scale\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # shift\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply layer normalization to the input.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Normalized input.\n",
    "        \"\"\"\n",
    "        mean = input.mean(-1, keepdim=True)\n",
    "        std = input.std(-1, keepdim=True)\n",
    "        output = self.alpha * (input - mean) / (std + self.eps) + self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Normalization is a technique to normalize the inputs across the features. It's applied after each sub-layer in the Transformer. Here's what it does:\n",
    "\n",
    "1. It calculates the mean and standard deviation of the input tensor along the last dimension (which represents the features).\n",
    "2. It normalizes the input using these statistics.\n",
    "3. It then applies a learnable scale (`alpha`) and shift (`bias`) to allow the network to undo the normalization if needed.\n",
    "\n",
    "Layer Normalization helps in stabilizing the learning process and reduces the training time by reducing the dependence of gradients on the scale of parameters or their initial values.\n",
    "\n",
    "### 6.4 Feedforward Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the feedforward network used in each Transformer layer.\n",
    "\n",
    "    Attributes:\n",
    "        linear1 (nn.Linear): First linear transformation.\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "        linear2 (nn.Linear): Second linear transformation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_embed: int, hidden_dim: int, dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the FeedforwardBlock.\n",
    "\n",
    "        Args:\n",
    "            dim_embed (int): The input and output dimension.\n",
    "            hidden_dim (int): The dimension of the hidden layer.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dim_embed, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(hidden_dim, dim_embed)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the feedforward network to the input.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output after applying the feedforward network.\n",
    "        \"\"\"\n",
    "        output = self.linear2(self.dropout(torch.relu(self.linear1(input))))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Feedforward Block is a simple neural network applied to each position separately and identically. It consists of:\n",
    "\n",
    "1. A linear transformation that expands the input dimension.\n",
    "2. A ReLU activation function.\n",
    "3. Dropout for regularization.\n",
    "4. Another linear transformation that projects back to the original input dimension.\n",
    "\n",
    "This block allows the model to process the information from the attention mechanism and introduce non-linearity into the model.\n",
    "\n",
    "### 6.5 Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a residual connection with layer normalization and dropout.\n",
    "\n",
    "    Attributes:\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "        layer_norm (LayerNormalization): Layer normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the ResidualConnection.\n",
    "\n",
    "        Args:\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, input: torch.Tensor, sublayer: callable) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the residual connection.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "            sublayer (callable): The sublayer to apply (e.g., attention or feedforward).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output after applying the residual connection.\n",
    "        \"\"\"\n",
    "        return input + self.dropout(sublayer(self.layer_norm(input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Residual Connection is a crucial component in the Transformer architecture. It does the following:\n",
    "\n",
    "1. Applies layer normalization to the input.\n",
    "2. Passes the normalized input through a sublayer (which could be attention or feedforward).\n",
    "3. Applies dropout to the output of the sublayer.\n",
    "4. Adds the result back to the original input (the residual connection).\n",
    "\n",
    "Residual connections help in training very deep networks by allowing gradients to flow directly through the network. The \"Add & Norm\" step in the original Transformer paper refers to this residual connection followed by layer normalization.\n",
    "\n",
    "In the next part, we'll continue with the Multi-Head Attention mechanism, which is at the core of the Transformer's power.\n",
    "\n",
    "### 6.6 Multi-Head Attention Block\n",
    "\n",
    "The Multi-Head Attention mechanism is the core innovation of the Transformer model. Let's break it down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Multi-Head Attention mechanism.\n",
    "\n",
    "    Attributes:\n",
    "        dim_k (int): Dimension of keys and queries.\n",
    "        dim_embed (int): Embedding dimension.\n",
    "        n_head (int): Number of attention heads.\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "        w_k (nn.Linear): Linear transformation for keys.\n",
    "        w_q (nn.Linear): Linear transformation for queries.\n",
    "        w_v (nn.Linear): Linear transformation for values.\n",
    "        w_o (nn.Linear): Linear transformation for output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_embed: int, n_head: int, dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the MultiHeadAttentionBlock.\n",
    "\n",
    "        Args:\n",
    "            dim_embed (int): Embedding dimension.\n",
    "            n_head (int): Number of attention heads.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert dim_embed % n_head == 0, 'Embedding dimension must be divisible by number of attention heads!'\n",
    "        self.dim_k = dim_embed // n_head\n",
    "        self.dim_embed = dim_embed\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.w_k = nn.Linear(dim_embed, dim_embed, bias=False)\n",
    "        self.w_q = nn.Linear(dim_embed, dim_embed, bias=False)\n",
    "        self.w_v = nn.Linear(dim_embed, dim_embed, bias=False)\n",
    "        self.w_o = nn.Linear(dim_embed, dim_embed, bias=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def self_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                       mask: torch.Tensor = None, dropout: nn.Dropout = None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute scaled dot-product attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Query tensor.\n",
    "            key (torch.Tensor): Key tensor.\n",
    "            value (torch.Tensor): Value tensor.\n",
    "            mask (torch.Tensor, optional): Attention mask. Defaults to None.\n",
    "            dropout (nn.Dropout, optional): Dropout layer. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Attention output and attention scores.\n",
    "        \"\"\"\n",
    "        d_k = query.shape[-1]\n",
    "        score = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -1e9)\n",
    "        score = torch.softmax(score, dim=-1)\n",
    "        if dropout is not None:\n",
    "            score = dropout(score)\n",
    "        return (score @ value), score\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            q (torch.Tensor): Query tensor.\n",
    "            k (torch.Tensor): Key tensor.\n",
    "            v (torch.Tensor): Value tensor.\n",
    "            mask (torch.Tensor, optional): Attention mask. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output after applying multi-head attention.\n",
    "        \"\"\"\n",
    "        bs = q.shape[0]\n",
    "\n",
    "        # Linear transformations and reshape\n",
    "        q = self.w_q(q).view(bs, -1, self.n_head, self.dim_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(bs, -1, self.n_head, self.dim_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(bs, -1, self.n_head, self.dim_k).transpose(1, 2)\n",
    "\n",
    "        # Apply attention\n",
    "        output, self.attention_score = self.self_attention(q, k, v, mask, self.dropout)\n",
    "\n",
    "        # Reshape and apply final linear transformation\n",
    "        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_head * self.dim_k)\n",
    "        return self.w_o(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multi-Head Attention mechanism allows the model to jointly attend to information from different representation subspaces at different positions. Here's a breakdown of how it works:\n",
    "\n",
    "1. The input is linearly projected to create queries, keys, and values using `w_q`, `w_k`, and `w_v` respectively.\n",
    "2. These projections are split into multiple heads (reshaped and transposed).\n",
    "3. Scaled dot-product attention is applied in parallel for each head.\n",
    "4. The results are concatenated and linearly transformed using `w_o`.\n",
    "\n",
    "The `self_attention` method implements the scaled dot-product attention mechanism:\n",
    "\n",
    "- It computes the dot product of queries and keys, scaled by the square root of their dimension.\n",
    "- If a mask is provided (used in the decoder to prevent attending to future tokens), it applies the mask.\n",
    "- The softmax function is applied to obtain attention weights.\n",
    "- These weights are used to compute a weighted sum of the values.\n",
    "\n",
    "### 6.7 Encoder Block & Encoder Module\n",
    "\n",
    "Now, let's look at how these components come together in an Encoder Block and then an Encoder module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single Encoder block of the Transformer.\n",
    "\n",
    "    Attributes:\n",
    "        attention_block (MultiHeadAttentionBlock): Multi-head attention mechanism.\n",
    "        ffn_block (FeedforwardBlock): Feedforward neural network.\n",
    "        residual_block (nn.ModuleList): List of residual connections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_block: MultiHeadAttentionBlock, ffn_block: FeedforwardBlock, dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the EncoderBlock.\n",
    "\n",
    "        Args:\n",
    "            attention_block (MultiHeadAttentionBlock): Multi-head attention mechanism.\n",
    "            ffn_block (FeedforwardBlock): Feedforward neural network.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attention_block = attention_block\n",
    "        self.ffn_block = ffn_block\n",
    "        self.residual_block = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, input: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Encoder block to the input.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "            mask (torch.Tensor): Attention mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output after applying the Encoder block.\n",
    "        \"\"\"\n",
    "        input = self.residual_block[0](input, lambda x: self.attention_block(x, x, x, mask))\n",
    "        output = self.residual_block[1](input, self.ffn_block)\n",
    "        return output\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the full Encoder of the Transformer.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): List of EncoderBlock layers.\n",
    "        norm (LayerNormalization): Final layer normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        \"\"\"\n",
    "        Initialize the Encoder.\n",
    "\n",
    "        Args:\n",
    "            layers (nn.ModuleList): List of EncoderBlock layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, input: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the full Encoder to the input.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "            mask (torch.Tensor): Attention mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output after applying the full Encoder.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer(input, mask)\n",
    "        return self.norm(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Encoder Block consists of:\n",
    "\n",
    "1. A Multi-Head Attention layer, where queries, keys, and values all come from the same input (hence \"self-attention\").\n",
    "2. A Feedforward layer.\n",
    "3. Residual connections and layer normalizations around each of these sub-layers.\n",
    "\n",
    "The Encoder applies each Encoder Block in sequence, with a final layer normalization at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8 Decoder Block & Decoder Module\n",
    "\n",
    "The Decoder Block is similar to the Encoder Block, but with an additional attention layer. Like the Ecnoder module, we will use multiple Decoder block to create Decoder module.\n",
    "\n",
    "The Decoder Block has three main components:\n",
    "\n",
    "1. A masked self-attention layer, where subsequent positions are prevented from attending to earlier positions.\n",
    "2. A cross-attention layer, which attends to the Encoder's output.\n",
    "3. A feedforward layer.\n",
    "\n",
    "Each of these is wrapped with a residual connection and layer normalization.\n",
    "\n",
    "The Decoder Module applies each Decoder Block in sequence, with a final layer normalization at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single Decoder block of the Transformer.\n",
    "\n",
    "    Attributes:\n",
    "        masked_attention_block (MultiHeadAttentionBlock): Masked self-attention mechanism.\n",
    "        attention_block (MultiHeadAttentionBlock): Cross-attention mechanism.\n",
    "        ffn_block (FeedforwardBlock): Feedforward neural network.\n",
    "        residual_block (nn.ModuleList): List of residual connections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, masked_attention_block: MultiHeadAttentionBlock, attention_block: MultiHeadAttentionBlock,\n",
    "                 ffn_block: FeedforwardBlock, dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the DecoderBlock.\n",
    "\n",
    "        Args:\n",
    "            masked_attention_block (MultiHeadAttentionBlock): Masked self-attention mechanism.\n",
    "            attention_block (MultiHeadAttentionBlock): Cross-attention mechanism.\n",
    "            ffn_block (FeedforwardBlock): Feedforward neural network.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.masked_attention_block = masked_attention_block\n",
    "        self.attention_block = attention_block\n",
    "        self.ffn_block = ffn_block\n",
    "        self.residual_block = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, input: torch.Tensor, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Decoder block to the input.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "            encoder_output (torch.Tensor): Output from the Encoder.\n",
    "            src_mask (torch.Tensor): Source attention mask.\n",
    "            tgt_mask (torch.Tensor): Target attention mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output after applying the Decoder block.\n",
    "        \"\"\"\n",
    "        input = self.residual_block[0](input, lambda x: self.masked_attention_block(x, x, x, tgt_mask))\n",
    "        input = self.residual_block[1](input, lambda x: self.attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        output = self.residual_block[2](input, self.ffn_block)\n",
    "        return output\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the full Decoder of the Transformer.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): List of DecoderBlock layers.\n",
    "        norm (LayerNormalization): Final layer normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        \"\"\"\n",
    "        Initialize the Decoder.\n",
    "\n",
    "        Args:\n",
    "            layers (nn.ModuleList): List of DecoderBlock layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, input: torch.Tensor, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the full Decoder to the input.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "            encoder_output (torch.Tensor): Output from the Encoder.\n",
    "            src_mask (torch.Tensor): Source attention mask.\n",
    "            tgt_mask (torch.Tensor): Target attention mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output after applying the full Decoder.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            input = layer(input, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.9 Final Projection\n",
    "\n",
    "After the Decoder, we need to project the output to our vocabulary size. This layer transforms the Decoder's output into log probabilities over our target vocabulary.\n",
    "\n",
    "In the next part, we'll put all these components together to create the full Transformer model and explain how to train and use it for translation tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects the Decoder output to vocabulary size and applies log softmax.\n",
    "\n",
    "    Attributes:\n",
    "        linear (nn.Linear): Linear transformation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, dim_embed: int):\n",
    "        \"\"\"\n",
    "        Initialize the Projection.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            dim_embed (int): Dimension of the input embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim_embed, vocab_size)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the projection to the input.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Log probabilities over the vocabulary.\n",
    "        \"\"\"\n",
    "        return torch.log_softmax(self.linear(input), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Assembling the Full Transformer Model\n",
    "\n",
    "Now that we have all the components, let's put them together to create our full Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the full Transformer model for sequence-to-sequence tasks.\n",
    "\n",
    "    Attributes:\n",
    "        src_emb (InputEmbedding): Input embedding for source language.\n",
    "        tgt_emb (InputEmbedding): Input embedding for target language.\n",
    "        src_pos_emb (PositionalEmbedding): Positional embedding for source language.\n",
    "        tgt_pos_emb (PositionalEmbedding): Positional embedding for target language.\n",
    "        encoder (Encoder): Encoder stack.\n",
    "        decoder (Decoder): Decoder stack.\n",
    "        projection (Projection): Final projection layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src_emb: InputEmbedding, tgt_emb: InputEmbedding,\n",
    "                 src_pos_emb: PositionalEmbedding, tgt_pos_emb: PositionalEmbedding,\n",
    "                 encoder: Encoder, decoder: Decoder, projection: Projection):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer.\n",
    "\n",
    "        Args:\n",
    "            src_emb (InputEmbedding): Input embedding for source language.\n",
    "            tgt_emb (InputEmbedding): Input embedding for target language.\n",
    "            src_pos_emb (PositionalEmbedding): Positional embedding for source language.\n",
    "            tgt_pos_emb (PositionalEmbedding): Positional embedding for target language.\n",
    "            encoder (Encoder): Encoder stack.\n",
    "            decoder (Decoder): Decoder stack.\n",
    "            projection (Projection): Final projection layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.src_emb = src_emb\n",
    "        self.tgt_emb = tgt_emb\n",
    "        self.src_pos_emb = src_pos_emb\n",
    "        self.tgt_pos_emb = tgt_pos_emb\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.projection = projection\n",
    "\n",
    "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode the source sequence.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Source sequence.\n",
    "            src_mask (torch.Tensor): Source mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded representation of the source sequence.\n",
    "        \"\"\"\n",
    "        src = self.src_emb(src)\n",
    "        src = self.src_pos_emb(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, tgt: torch.Tensor, tgt_mask: torch.Tensor, src_mask: torch.Tensor, encoder_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode the target sequence.\n",
    "\n",
    "        Args:\n",
    "            tgt (torch.Tensor): Target sequence.\n",
    "            tgt_mask (torch.Tensor): Target mask.\n",
    "            src_mask (torch.Tensor): Source mask.\n",
    "            encoder_output (torch.Tensor): Output from the encoder.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Decoded representation of the target sequence.\n",
    "        \"\"\"\n",
    "        tgt = self.tgt_emb(tgt)\n",
    "        tgt = self.tgt_pos_emb(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, decoder_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Project the decoder output to vocabulary space.\n",
    "\n",
    "        Args:\n",
    "            decoder_output (torch.Tensor): Output from the decoder.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Log probabilities over the target vocabulary.\n",
    "        \"\"\"\n",
    "        return self.projection(decoder_output)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the Transformer.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Source sequence.\n",
    "            tgt (torch.Tensor): Target sequence.\n",
    "            src_mask (torch.Tensor): Source mask.\n",
    "            tgt_mask (torch.Tensor): Target mask.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Log probabilities over the target vocabulary for each position.\n",
    "        \"\"\"\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        decoder_output = self.decode(tgt, tgt_mask, src_mask, encoder_output)\n",
    "        return self.project(decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Transformer` class combines all the components we've built so far. The forward pass:\n",
    "\n",
    "1. Encodes the source sequence.\n",
    "2. Decodes the target sequence, using the encoded source.\n",
    "3. Projects the decoder output to get log probabilities over the target vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Creating the Transformer Model\n",
    "\n",
    "Now, let's create a function to instantiate our Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_model(\n",
    "        src_vocab_size: int,\n",
    "        tgt_vocab_size: int,\n",
    "        src_max_seq_len: int,\n",
    "        tgt_max_seq_len: int,\n",
    "        dim_embed: int = 256,\n",
    "        n_layer: int = 8,\n",
    "        n_head: int = 6,\n",
    "        dropout: float = 0.2,\n",
    "        ffn_hidden_dim: int = 1024\n",
    "    ) -> Transformer:\n",
    "    \"\"\"\n",
    "    Create a Transformer model with the specified parameters.\n",
    "\n",
    "    Args:\n",
    "        src_vocab_size (int): Size of the source vocabulary.\n",
    "        tgt_vocab_size (int): Size of the target vocabulary.\n",
    "        src_max_seq_len (int): Maximum length of source sequences.\n",
    "        tgt_max_seq_len (int): Maximum length of target sequences.\n",
    "        dim_embed (int, optional): Embedding dimension. Defaults to 256.\n",
    "        n_layer (int, optional): Number of encoder/decoder layers. Defaults to 8.\n",
    "        n_head (int, optional): Number of attention heads. Defaults to 6.\n",
    "        dropout (float, optional): Dropout rate. Defaults to 0.2.\n",
    "        ffn_hidden_dim (int, optional): Hidden dimension of feedforward networks. Defaults to 1024.\n",
    "\n",
    "    Returns:\n",
    "        Transformer: Instantiated Transformer model.\n",
    "    \"\"\"\n",
    "    src_embedding = InputEmbedding(src_vocab_size, dim_embed)\n",
    "    tgt_embedding = InputEmbedding(tgt_vocab_size, dim_embed)\n",
    "    src_pos_embedding = PositionalEmbedding(dim_embed, src_max_seq_len, dropout)\n",
    "    tgt_pos_embedding = PositionalEmbedding(dim_embed, tgt_max_seq_len, dropout)\n",
    "\n",
    "    encoder_blocks = []\n",
    "    decoder_blocks = []\n",
    "    for _ in range(n_layer):\n",
    "        encoder_attention = MultiHeadAttentionBlock(dim_embed, n_head, dropout)\n",
    "        encoder_ffn = FeedforwardBlock(dim_embed, ffn_hidden_dim, dropout)\n",
    "        encoder_block = EncoderBlock(encoder_attention, encoder_ffn, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "        decoder_masked_attention = MultiHeadAttentionBlock(dim_embed, n_head, dropout)\n",
    "        decoder_attention = MultiHeadAttentionBlock(dim_embed, n_head, dropout)\n",
    "        decoder_ffn = FeedforwardBlock(dim_embed, ffn_hidden_dim, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_masked_attention, decoder_attention, decoder_ffn, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "    projection = Projection(tgt_vocab_size, dim_embed)\n",
    "\n",
    "    transformer = Transformer(\n",
    "        src_embedding,\n",
    "        tgt_embedding,\n",
    "        src_pos_embedding,\n",
    "        tgt_pos_embedding,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        projection\n",
    "    )\n",
    "\n",
    "    # Initialize parameters with Xavier uniform distribution\n",
    "    for param in transformer.parameters():\n",
    "        if param.dim() > 1:\n",
    "            nn.init.xavier_uniform_(param)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training the Model\n",
    "\n",
    "Now that we have our model, let's set up the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, Loss: 6.4170, Time: 89.81 seconds\n",
      "New best model saved with loss: 6.4170\n",
      "Epoch: 2/100, Loss: 5.5120, Time: 95.87 seconds\n",
      "New best model saved with loss: 5.5120\n",
      "Epoch: 3/100, Loss: 5.2158, Time: 99.55 seconds\n",
      "New best model saved with loss: 5.2158\n",
      "Epoch: 4/100, Loss: 4.9875, Time: 100.15 seconds\n",
      "New best model saved with loss: 4.9875\n",
      "Epoch: 5/100, Loss: 4.8123, Time: 99.99 seconds\n",
      "New best model saved with loss: 4.8123\n",
      "Epoch: 6/100, Loss: 4.6637, Time: 99.78 seconds\n",
      "New best model saved with loss: 4.6637\n",
      "Epoch: 7/100, Loss: 4.5310, Time: 101.85 seconds\n",
      "New best model saved with loss: 4.5310\n",
      "Epoch: 8/100, Loss: 4.4090, Time: 100.23 seconds\n",
      "New best model saved with loss: 4.4090\n",
      "Epoch: 9/100, Loss: 4.3023, Time: 101.62 seconds\n",
      "New best model saved with loss: 4.3023\n",
      "Epoch: 10/100, Loss: 4.1951, Time: 100.22 seconds\n",
      "New best model saved with loss: 4.1951\n",
      "Epoch: 11/100, Loss: 4.0931, Time: 101.04 seconds\n",
      "New best model saved with loss: 4.0931\n",
      "Epoch: 12/100, Loss: 4.0014, Time: 101.33 seconds\n",
      "New best model saved with loss: 4.0014\n",
      "Epoch: 13/100, Loss: 3.9121, Time: 99.13 seconds\n",
      "New best model saved with loss: 3.9121\n",
      "Epoch: 14/100, Loss: 3.8251, Time: 100.35 seconds\n",
      "New best model saved with loss: 3.8251\n",
      "Epoch: 15/100, Loss: 3.7389, Time: 100.74 seconds\n",
      "New best model saved with loss: 3.7389\n",
      "Epoch: 16/100, Loss: 3.6632, Time: 98.83 seconds\n",
      "New best model saved with loss: 3.6632\n",
      "Epoch: 17/100, Loss: 3.5814, Time: 91.71 seconds\n",
      "New best model saved with loss: 3.5814\n",
      "Epoch: 18/100, Loss: 3.5072, Time: 85.46 seconds\n",
      "New best model saved with loss: 3.5072\n",
      "Epoch: 19/100, Loss: 3.4340, Time: 89.78 seconds\n",
      "New best model saved with loss: 3.4340\n",
      "Epoch: 20/100, Loss: 3.3675, Time: 88.42 seconds\n",
      "New best model saved with loss: 3.3675\n",
      "Epoch: 21/100, Loss: 3.2990, Time: 88.52 seconds\n",
      "New best model saved with loss: 3.2990\n",
      "Epoch: 22/100, Loss: 3.2325, Time: 91.11 seconds\n",
      "New best model saved with loss: 3.2325\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os\n",
    "\n",
    "def create_mask(src: torch.Tensor, tgt: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create masks for source and target sequences.\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): Source sequence.\n",
    "        tgt (torch.Tensor): Target sequence.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: Source and target masks.\n",
    "    \"\"\"\n",
    "    src_mask = (src != src_token_to_id['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "    tgt_mask = (tgt != tgt_token_to_id['<pad>']).unsqueeze(1).unsqueeze(3)\n",
    "\n",
    "    tgt_len = tgt.size(1)\n",
    "    tgt_submask = torch.tril(torch.ones((tgt_len, tgt_len), device=device)).bool()\n",
    "    tgt_mask = tgt_mask & tgt_submask\n",
    "\n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "def train_epoch(model: nn.Module, optimizer: torch.optim.Optimizer, dataloader: DataLoader, loss_fn: nn.Module, device: torch.device, log_file) -> float:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The Transformer model.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer.\n",
    "        dataloader (DataLoader): DataLoader for training data.\n",
    "        loss_fn (nn.Module): The loss function.\n",
    "        device (torch.device): The device to train on.\n",
    "        log_file: File object for logging.\n",
    "\n",
    "    Returns:\n",
    "        float: Average loss for this epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, (src, tgt) in enumerate(dataloader):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        src_mask, tgt_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask)\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_output.contiguous().reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Log the loss after each iteration\n",
    "        log_file.write(f\"Iteration {i+1}, Loss: {loss.item():.4f}\\n\")\n",
    "        log_file.flush()  # Ensure the loss is written to the file immediately\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, epoch_time\n",
    "\n",
    "# Set up the model and training\n",
    "transformer_model = create_transformer_model(\n",
    "    vocab_src_size,\n",
    "    vocab_tgt_size,\n",
    "    max_src_seq_len,\n",
    "    max_tgt_seq_len,\n",
    "    parameters['dim_embed'],\n",
    "    parameters['n_layers'],\n",
    "    parameters['n_heads'],\n",
    "    parameters['dropout'],\n",
    "    parameters['ffn_hidden_dim']\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=parameters['lr'], betas=(0.9, 0.98), eps=1e-9)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tgt_token_to_id['<pad>'])\n",
    "dataloader = DataLoader(dataset, batch_size=parameters['bs'], shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Create model save directory if it doesn't exist\n",
    "os.makedirs(parameters['model_path'], exist_ok=True)\n",
    "\n",
    "# Construct the full paths for the model files\n",
    "best_model_path = os.path.join(parameters['model_path'], 'best_transformer_model.pth')\n",
    "latest_model_path = os.path.join(parameters['model_path'], 'latest_transformer_model.pth')\n",
    "\n",
    "best_loss = float('inf')\n",
    "no_improvement_count = 0\n",
    "max_no_improvement = 3\n",
    "\n",
    "# Open the log file\n",
    "with open(\"log.txt\", \"w\") as log_file:\n",
    "    log_file.write(\"Training started\\n\")\n",
    "    \n",
    "    for epoch in range(parameters['n_epochs']):\n",
    "        train_loss, epoch_time = train_epoch(transformer_model, optimizer, dataloader, loss_fn, device, log_file)\n",
    "        log_message = f'Epoch: {epoch+1}/{parameters[\"n_epochs\"]}, Loss: {train_loss:.4f}, Time: {epoch_time:.2f} seconds'\n",
    "        print(log_message)\n",
    "        log_file.write(log_message + \"\\n\")\n",
    "        \n",
    "        # Check for improvement\n",
    "        if round(train_loss, 2) < round(best_loss, 2):\n",
    "            best_loss = train_loss\n",
    "            no_improvement_count = 0\n",
    "            torch.save(transformer_model.state_dict(), best_model_path)\n",
    "            save_message = f\"New best model saved with loss: {best_loss:.4f}\"\n",
    "            print(save_message)\n",
    "            log_file.write(save_message + \"\\n\")\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "        \n",
    "        # Save the latest model (useful for resuming training)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': transformer_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_loss,\n",
    "        }, latest_model_path)\n",
    "        \n",
    "        log_file.flush()\n",
    "\n",
    "        # Check for early stopping\n",
    "        if no_improvement_count >= max_no_improvement:\n",
    "            print(f\"Early stopping after {no_improvement_count} epochs without improvement.\")\n",
    "            log_file.write(f\"Early stopping after {no_improvement_count} epochs without improvement.\\n\")\n",
    "            break\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    log_file.write(\"Training complete.\\n\")\n",
    "\n",
    "print(f\"Best model saved to {best_model_path}\")\n",
    "print(f\"Latest model checkpoint saved to {latest_model_path}\")\n",
    "print(f\"Total epochs: {epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training loop does the following for each epoch:\n",
    "\n",
    "1. Iterates over batches of data.\n",
    "2. Creates appropriate masks for source and target sequences.\n",
    "3. Performs a forward pass through the model.\n",
    "4. Calculates the loss.\n",
    "5. Performs backpropagation and updates the model parameters.\n",
    "\n",
    "After training, we save the model weights for later use.\n",
    "\n",
    "## 10. Using the Model for Translation\n",
    "\n",
    "Finally, let's create a function to use our trained model for translation.\n",
    "\n",
    "This `translate` function:\n",
    "\n",
    "1. Encodes the source sequence.\n",
    "2. Generates the translation one token at a time, using the previously generated tokens as input for the next prediction.\n",
    "3. Stops when it generates an end-of-sequence token or reaches the maximum length.\n",
    "\n",
    "With this, we've completed our implementation of a Transformer model for machine translation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Hello, how are you?\n",
      "Translation: A vous le sentir .\n"
     ]
    }
   ],
   "source": [
    "def translate(model: nn.Module, src: torch.Tensor, max_len: int = 50) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Translate a source sequence using the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained Transformer model.\n",
    "        src (torch.Tensor): Source sequence tensor.\n",
    "        max_len (int, optional): Maximum length of the translated sequence. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Translated sequence as tensor of token IDs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    src_mask = (src != src_token_to_id['<pad>']).unsqueeze(-2)\n",
    "    enc_output = model.encode(src, src_mask)\n",
    "\n",
    "    tgt = torch.ones(1, 1).fill_(tgt_token_to_id['<sos>']).type_as(src).long()\n",
    "    for i in range(max_len-1):\n",
    "        tgt_mask = (tgt != tgt_token_to_id['<pad>']).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & torch.tril(torch.ones((1, tgt.size(1), tgt.size(1)), device=device)).bool()\n",
    "\n",
    "        out = model.decode(tgt, tgt_mask, src_mask, enc_output)\n",
    "        prob = model.project(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        tgt = torch.cat([tgt, torch.ones(1, 1).type_as(src).long().fill_(next_word)], dim=1)\n",
    "\n",
    "        if next_word == tgt_token_to_id['<eos>']:\n",
    "            break\n",
    "\n",
    "    return tgt.squeeze(0)\n",
    "\n",
    "# Example usage\n",
    "src_sentence = \"Hello, how are you?\"\n",
    "src_tokens = ['<sos>'] + word_tokenize(src_sentence) + ['<eos>']\n",
    "src_ids = [src_token_to_id.get(token, src_token_to_id['<unk>']) for token in src_tokens]\n",
    "src_tensor = torch.tensor(src_ids).unsqueeze(0).to(device)\n",
    "\n",
    "translated_ids = translate(transformer_model, src_tensor)\n",
    "translated_tokens = [tgt_id_to_token[id.item()] for id in translated_ids if id.item() not in [tgt_token_to_id['<sos>'], tgt_token_to_id['<eos>'], tgt_token_to_id['<pad>']]]\n",
    "translated_sentence = ' '.join(translated_tokens)\n",
    "\n",
    "print(f\"Source: {src_sentence}\")\n",
    "print(f\"Translation: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation & Translation\n",
    "\n",
    "To properly assess our model's performance, we should use established evaluation metrics for machine translation. The most common metric is BLEU (Bilingual Evaluation Understudy) score. Let's implement a function to calculate BLEU scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pdeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Sentence: J'aime la programmation. \t Translated Sentence: Je veux supporter ce sujet .\n",
      "Reference Sentence: Elle est une bonne professeure. \t Translated Sentence: Elle est une bonne maîtresse .\n",
      "Reference Sentence: Il joue de la guitare. \t Translated Sentence: Il fait de ce devoir .\n",
      "Reference Sentence: Nous sommes allés à la plage. \t Translated Sentence: Nous avons fait ce sujet .\n",
      "Reference Sentence: J'ai deux frères. \t Translated Sentence: J'ai deux ans .\n",
      "Reference Sentence: Elle parle trois langues. \t Translated Sentence: Elle est puis , le reste de la Mr .\n",
      "Reference Sentence: Il vient de France. \t Translated Sentence: Il est de la somme de\n",
      "Reference Sentence: Nous allons au cinéma. \t Translated Sentence: Nous allons nous établir dans ce sujet .\n",
      "Reference Sentence: Je bois du café tous les matins. \t Translated Sentence: Je passai tout le café du matin .\n",
      "Reference Sentence: Elle lit un livre tous les jours. \t Translated Sentence: Elle mit leur livre pendant ce temps à parler .\n",
      "Reference Sentence: Il aime cuisiner. \t Translated Sentence: Il fait cela » .\n",
      "Reference Sentence: Nous apprenons le français. \t Translated Sentence: Nous sommes en effet , pourquoi m ’ en ont été parler .\n",
      "Reference Sentence: Je travaille comme ingénieur logiciel. \t Translated Sentence: Je me mis à cela un peu avez .\n",
      "Reference Sentence: Elle étudie la médecine. \t Translated Sentence: Elle pense .\n",
      "Reference Sentence: Il est musicien professionnel. \t Translated Sentence: Il est un peu\n",
      "Reference Sentence: Nous rendons visite à nos grands-parents. \t Translated Sentence: Nous sommes tous les voitures entre des voitures .\n",
      "Reference Sentence: Je préfère le thé au café. \t Translated Sentence: J ’ ai lieu de prendre le thé .\n",
      "Reference Sentence: Elle est végétarienne. \t Translated Sentence: Elle est inutile de supporter .\n",
      "Reference Sentence: Il est un coureur. \t Translated Sentence: Il est facile de supporter .\n",
      "Reference Sentence: Nous partons en vacances. \t Translated Sentence: Nous sommes arrivés en chercher .\n",
      "Test BLEU score: 0.0269\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def evaluate_bleu(model: nn.Module, test_data: list[tuple[str, str]]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model using BLEU score.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained Transformer model.\n",
    "        test_data (List[Tuple[str, str]]): List of (source, target) sentence pairs.\n",
    "\n",
    "    Returns:\n",
    "        float: Average BLEU score across all test samples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    bleu_scores = []\n",
    "\n",
    "    for src, ref in test_data:\n",
    "        src_tokens = ['<sos>'] + word_tokenize(src) + ['<eos>']\n",
    "        src_ids = [src_token_to_id.get(token, src_token_to_id['<unk>']) for token in src_tokens]\n",
    "        src_tensor = torch.tensor(src_ids).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            translated_ids = translate(model, src_tensor)\n",
    "\n",
    "        translated_tokens = [tgt_id_to_token[id.item()] for id in translated_ids\n",
    "                             if id.item() not in [tgt_token_to_id['<sos>'], tgt_token_to_id['<eos>'], tgt_token_to_id['<pad>']]]\n",
    "\n",
    "        \n",
    "        translated_sentence = ' '.join(translated_tokens)\n",
    "        \n",
    "        reference_tokens = word_tokenize(ref)\n",
    "\n",
    "        print(f\"Reference Sentence: {ref} \\t Translated Sentence: {translated_sentence}\")\n",
    "\n",
    "        bleu = sentence_bleu([reference_tokens], translated_tokens)\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "    return sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Sample validaion data\n",
    "sample_data = [\n",
    "    (\"I love programming.\", \"J'aime la programmation.\"),\n",
    "    (\"She is a good teacher.\", \"Elle est une bonne professeure.\"),\n",
    "    (\"He plays the guitar.\", \"Il joue de la guitare.\"),\n",
    "    (\"We went to the beach.\", \"Nous sommes allés à la plage.\"),\n",
    "    (\"I have two brothers.\", \"J'ai deux frères.\"),\n",
    "    (\"She speaks three languages.\", \"Elle parle trois langues.\"),\n",
    "    (\"He is from France.\", \"Il vient de France.\"),\n",
    "    (\"We are going to the cinema.\", \"Nous allons au cinéma.\"),\n",
    "    (\"I drink coffee every morning.\", \"Je bois du café tous les matins.\"),\n",
    "    (\"She reads a book every day.\", \"Elle lit un livre tous les jours.\"),\n",
    "    (\"He likes to cook.\", \"Il aime cuisiner.\"),\n",
    "    (\"We are learning French.\", \"Nous apprenons le français.\"),\n",
    "    (\"I work as a software engineer.\", \"Je travaille comme ingénieur logiciel.\"),\n",
    "    (\"She studies medicine.\", \"Elle étudie la médecine.\"),\n",
    "    (\"He is a professional musician.\", \"Il est musicien professionnel.\"),\n",
    "    (\"We are visiting our grandparents.\", \"Nous rendons visite à nos grands-parents.\"),\n",
    "    (\"I prefer tea to coffee.\", \"Je préfère le thé au café.\"),\n",
    "    (\"She is a vegetarian.\", \"Elle est végétarienne.\"),\n",
    "    (\"He is a runner.\", \"Il est un coureur.\"),\n",
    "    (\"We are going on a vacation.\", \"Nous partons en vacances.\")\n",
    "]\n",
    "\n",
    "# Split the sample data into source and target sentences\n",
    "source_sentences, target_sentences = zip(*sample_data)\n",
    "\n",
    "# Create a test dataset\n",
    "test_dataset = list(zip(source_sentences, target_sentences))\n",
    "\n",
    "# Evaluate test_dataset\n",
    "test_bleu = evaluate_bleu(transformer_model, test_dataset)\n",
    "print(f\"Test BLEU score: {test_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MJ]",
   "language": "python",
   "name": "conda-env-MJ-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
